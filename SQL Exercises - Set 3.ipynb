{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Notebook Information '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Notebook Information \"\"\"\n",
    "# References: https://github.com/XD-DENG/SQL-exercise \n",
    "# Workbook Title: PySpark SQL Exercises - Set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Setup\n",
    "--------------------------------------------\n",
    "#### Installing relevant libraries; Instantiating a PySpark session; Creating a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Importing libraries \"\"\"\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Instantiate a SparkContext \"\"\"\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "## Print the Spark version\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Creating a SparkSession \"\"\" \n",
    "spark = SparkSession.builder.appName('JoinsTutorial').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Dataframes\n",
    "--------------------------------------------\n",
    "#### Creating tables as PySpark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Building the schema \"\"\"\n",
    "# Table 1 - Warehouses Table \n",
    "data1 = [[1,'Chicago',3],\n",
    "        [2,'Chicago',4], \n",
    "        [3,'New York',7],\n",
    "        [4,'Los Angeles',2],\n",
    "        [5,'San Francisco',8]]\n",
    "  \n",
    "# specify column names\n",
    "columns = ['Code', 'Location', 'Capacity']\n",
    "  \n",
    "# creating a df1 from the lists of data\n",
    "df1 = spark.createDataFrame(data1, columns)\n",
    "\n",
    "# Table 2 - Boxes Table \n",
    "data2 = [['0MN7','Rocks',180,3],\n",
    "         ['4H8P','Rocks',250,1],\n",
    "         ['4RT3','Scissors',190,4],\n",
    "         ['7G3H','Rocks',200,1],\n",
    "         ['8JN6','Papers',75,1],\n",
    "         ['8Y6U','Papers',50,3],\n",
    "         ['9J6F','Papers',175,2],\n",
    "         ['LL08','Rocks',140,4],\n",
    "         ['P0H6','Scissors',125,1],\n",
    "         ['P2T6','Scissors',150,2],\n",
    "         ['TU55','Papers',90,5]]\n",
    "  \n",
    "# specify column names\n",
    "columns = ['Code', 'Contents', 'Value', 'Warehouse']\n",
    "  \n",
    "# creating a df1 from the lists of data\n",
    "df2 = spark.createDataFrame(data2, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Creating Temporary Tables to be used in spark.sql \"\"\"\n",
    "## setting temporary views by creating dataframes\n",
    "# creating a view for df1 named Manufacturers\n",
    "df1.createOrReplaceTempView(\"Warehouses\")\n",
    "  \n",
    "# creating a view for df2 named Products\n",
    "df2.createOrReplaceTempView(\"Boxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Code: long (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Capacity: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Checking the column types for Table 1 - Warehouses \"\"\"\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Contents: string (nullable = true)\n",
      " |-- Value: long (nullable = true)\n",
      " |-- Warehouse: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Checking the column types for Table 2 - Boxes \"\"\"\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+\n",
      "|Code|Location|Capacity|\n",
      "+----+--------+--------+\n",
      "|1   |Chicago |3       |\n",
      "|2   |Chicago |4       |\n",
      "|3   |New York|7       |\n",
      "+----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Checking the Warehouses table \"\"\" \n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM Warehouses\n",
    "        LIMIT 3\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+---------+\n",
      "|Code|Contents|Value|Warehouse|\n",
      "+----+--------+-----+---------+\n",
      "|0MN7|Rocks   |180  |3        |\n",
      "|4H8P|Rocks   |250  |1        |\n",
      "|4RT3|Scissors|190  |4        |\n",
      "+----+--------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Checking the Boxes table \"\"\" \n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM Boxes\n",
    "        LIMIT 3\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries - Set 3\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------+\n",
      "|Code|Location     |Capacity|\n",
      "+----+-------------+--------+\n",
      "|1   |Chicago      |3       |\n",
      "|2   |Chicago      |4       |\n",
      "|3   |New York     |7       |\n",
      "|4   |Los Angeles  |2       |\n",
      "|5   |San Francisco|8       |\n",
      "+----+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query 3.1 \"\"\"\n",
    "\n",
    "# -- Select all warehouses.\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM Warehouses\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+---------+\n",
      "|Code|Contents|Value|Warehouse|\n",
      "+----+--------+-----+---------+\n",
      "|0MN7|Rocks   |180  |3        |\n",
      "|4H8P|Rocks   |250  |1        |\n",
      "|4RT3|Scissors|190  |4        |\n",
      "|7G3H|Rocks   |200  |1        |\n",
      "|8JN6|Papers  |75   |1        |\n",
      "|8Y6U|Papers  |50   |3        |\n",
      "|9J6F|Papers  |175  |2        |\n",
      "|LL08|Rocks   |140  |4        |\n",
      "|P0H6|Scissors|125  |1        |\n",
      "|P2T6|Scissors|150  |2        |\n",
      "|TU55|Papers  |90   |5        |\n",
      "+----+--------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query 3.2 \"\"\"\n",
    "\n",
    "# -- Select all boxes with a value larger than $150.\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM Boxes\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Contents|\n",
      "+--------+\n",
      "|Rocks   |\n",
      "|Papers  |\n",
      "|Scissors|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query 3.3 \"\"\"\n",
    "\n",
    "# -- Select all distinct contents in all the boxes.\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT DISTINCT Contents\n",
    "        FROM Boxes\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Contents|\n",
      "+--------+\n",
      "|Rocks   |\n",
      "|Papers  |\n",
      "|Scissors|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query 3.4 \"\"\"\n",
    "\n",
    "# -- Select the average value of all the boxes.\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT DISTINCT Contents\n",
    "        FROM Boxes\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Average           |\n",
      "+------------------+\n",
      "|147.72727272727272|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query 3.5 \"\"\"\n",
    "\n",
    "# -- Select the warehouse code and the average value of the boxes in each warehouse.\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT AVG(value) AS Average\n",
    "        FROM Boxes\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|warehouse|Average|\n",
      "+---------+-------+\n",
      "|1        |162.5  |\n",
      "|2        |162.5  |\n",
      "|4        |165.0  |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query 3.6 \"\"\"\n",
    "\n",
    "# -- Same as previous exercise, but select only those warehouses where the average value of the boxes is greater than 150.\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT warehouse, AVG(value) AS Average\n",
    "        FROM Boxes\n",
    "        GROUP BY warehouse\n",
    "        HAVING AVG(value) > 150\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+---------+----+-------------+--------+\n",
      "|Code|Contents|Value|Warehouse|Code|Location     |Capacity|\n",
      "+----+--------+-----+---------+----+-------------+--------+\n",
      "|TU55|Papers  |90   |5        |5   |San Francisco|8       |\n",
      "|4H8P|Rocks   |250  |1        |1   |Chicago      |3       |\n",
      "|7G3H|Rocks   |200  |1        |1   |Chicago      |3       |\n",
      "|8JN6|Papers  |75   |1        |1   |Chicago      |3       |\n",
      "|P0H6|Scissors|125  |1        |1   |Chicago      |3       |\n",
      "|0MN7|Rocks   |180  |3        |3   |New York     |7       |\n",
      "|8Y6U|Papers  |50   |3        |3   |New York     |7       |\n",
      "|9J6F|Papers  |175  |2        |2   |Chicago      |4       |\n",
      "|P2T6|Scissors|150  |2        |2   |Chicago      |4       |\n",
      "|4RT3|Scissors|190  |4        |4   |Los Angeles  |2       |\n",
      "|LL08|Rocks   |140  |4        |4   |Los Angeles  |2       |\n",
      "+----+--------+-----+---------+----+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query 3.7 \"\"\"\n",
    "\n",
    "# -- Select the code of each box, along with the name of the city the box is located in.\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM Boxes AS A INNER JOIN Warehouses AS B ON A.Warehouse = B.Code \n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|Warehouse|Boxes_count|\n",
      "+---------+-----------+\n",
      "|5        |1          |\n",
      "|1        |4          |\n",
      "|3        |2          |\n",
      "|2        |2          |\n",
      "|4        |2          |\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query 3.8 \"\"\"\n",
    "\n",
    "# -- Select the warehouse codes, along with the number of boxes in each warehouse. \n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT Warehouse, COUNT(*) AS Boxes_count\n",
    "        FROM Boxes\n",
    "        GROUP BY Warehouse\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Code|\n",
      "+----+\n",
      "|1   |\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query 3.9 \"\"\" # CONTINUE FROM HERE \n",
    "\n",
    "# -- Select the codes of all warehouses that are saturated (a warehouse is saturated if the number of boxes in it is larger than the warehouse's capacity).\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT Code\n",
    "        FROM Warehouses\n",
    "        WHERE Capacity < (SELECT COUNT(*)\n",
    "                          FROM Boxes\n",
    "                          WHERE Warehouse = Warehouses.Code)\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Code|\n",
      "+----+\n",
      "|4H8P|\n",
      "|7G3H|\n",
      "|8JN6|\n",
      "|P0H6|\n",
      "|9J6F|\n",
      "|P2T6|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query 3.10 - Join Approach \"\"\" \n",
    "\n",
    "# -- Select the codes and location of all the boxes located in Chicago with the\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT A.Code\n",
    "        FROM Boxes AS A INNER JOIN Warehouses AS B ON A.Warehouse = B.Code\n",
    "        WHERE B.Location = 'Chicago'\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Code|\n",
      "+----+\n",
      "|4H8P|\n",
      "|7G3H|\n",
      "|8JN6|\n",
      "|P0H6|\n",
      "|9J6F|\n",
      "|P2T6|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query 3.10 - Subquery Approach \"\"\" # CONTINUE FROM HERE\n",
    "\n",
    "# -- Select the codes of all the boxes located in Chicago with the\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT Code\n",
    "        FROM Boxes\n",
    "        WHERE Warehouse IN (SELECT Code\n",
    "                            FROM Warehouses\n",
    "                            WHERE Location = 'Chicago')\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Average_capacity  |\n",
      "+------------------+\n",
      "|3.3333333333333335|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Query 3.11 - Join Approach \"\"\" \n",
    "\n",
    "# -- Select the average capacity of the boxes that are located in Chicago's warehouse\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT AVG(Capacity) AS Average_capacity\n",
    "        FROM Boxes AS A INNER JOIN Warehouses AS B ON A.Warehouse = B.Code\n",
    "        WHERE B.Location = 'Chicago'\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6368a34afb731bb0f841c9450d2a8e61658b3b17cc345702a1e169cc2d30af5c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

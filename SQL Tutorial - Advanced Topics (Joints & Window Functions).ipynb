{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Setup\n",
    "--------------------------------------------\n",
    "#### Installing relevant libraries; Instantiating a PySpark session; Creating a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Importing libraries \"\"\"\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Instantiate a SparkContext \"\"\"\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "## Print the Spark version\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Creating a SparkSession \"\"\" \n",
    "spark = SparkSession.builder.appName('JoinsTutorial').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0 - Setting PySpark Dataframes & Registering Temporary Tables\n",
    "--------------------------------------------\n",
    "\n",
    "##### Overview on Databases Schema - Each DataFrame (df1, df2, df3, df4) contains 30 rows of data. You can perform joins between these tables based on your requirement. For example, you might join the Assignment table with the Employees table based on the Employee_ID to get employee details for each assignment. Similarly, you might join Assignment with Departments and Projects on Department_ID and Project_ID respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of employee data\n",
    "data1 = [[str(i), \"Employee_\" + str(i), \"Company_\" + str(i % 3), i * 5000] for i in range(1, 31)]\n",
    "  \n",
    "# Specify column names\n",
    "columns1 = ['ID', 'Name', 'Company', 'Salary']\n",
    "  \n",
    "# Create a DataFrame from the list of data\n",
    "df1 = spark.createDataFrame(data1, columns1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of department data\n",
    "data2 = [[str(i), \"Department_\" + str(i), \"Company_\" + str(i % 3)] for i in range(1, 31)]\n",
    "  \n",
    "# Specify column names\n",
    "columns2 = ['Department_ID', 'Department_Name', 'Company']\n",
    "  \n",
    "# Create a DataFrame from the list of data\n",
    "df2 = spark.createDataFrame(data2, columns2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of project data\n",
    "data3 = [[str(i), \"Project_\" + str(i), \"Company_\" + str(i % 3)] for i in range(1, 31)]\n",
    "  \n",
    "# Specify column names\n",
    "columns3 = ['Project_ID', 'Project_Name', 'Company']\n",
    "  \n",
    "# Create a DataFrame from the list of data\n",
    "df3 = spark.createDataFrame(data3, columns3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of assignment data\n",
    "data4 = [[str(i), str(i), str((i+5) % 30 + 1), str((i+10) % 30 + 1)] for i in range(1, 31)]\n",
    "  \n",
    "# Specify column names\n",
    "columns4 = ['Assignment_ID', 'Employee_ID', 'Department_ID', 'Project_ID']\n",
    "  \n",
    "# Create a DataFrame from the list of data\n",
    "df4 = spark.createDataFrame(data4, columns4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df1.createOrReplaceTempView(\"employees\")\n",
    "df2.createOrReplaceTempView(\"departments\")\n",
    "df3.createOrReplaceTempView(\"projects\")\n",
    "df4.createOrReplaceTempView(\"assignments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 - Advanced Joins\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-------------+----------+-------------+----------------+---------------+------------------+------------+---------------+---------------+\n",
      "|Assignment_ID|Employee_ID|Department_ID|Project_ID|Employee_Name|Employee_Company|Department_Name|Department_Company|Project_Name|Project_Company|Employee_Salary|\n",
      "+-------------+-----------+-------------+----------+-------------+----------------+---------------+------------------+------------+---------------+---------------+\n",
      "|26           |26         |2            |7         |Employee_26  |Company_2       |Department_2   |Company_2         |Project_7   |Company_1      |130000         |\n",
      "|30           |30         |6            |11        |Employee_30  |Company_0       |Department_6   |Company_0         |Project_11  |Company_2      |150000         |\n",
      "|18           |18         |24           |29        |Employee_18  |Company_0       |Department_24  |Company_0         |Project_29  |Company_2      |90000          |\n",
      "|22           |22         |28           |3         |Employee_22  |Company_1       |Department_28  |Company_1         |Project_3   |Company_0      |110000         |\n",
      "|19           |19         |25           |30        |Employee_19  |Company_1       |Department_25  |Company_1         |Project_30  |Company_0      |95000          |\n",
      "+-------------+-----------+-------------+----------+-------------+----------------+---------------+------------------+------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, calculate the average salary and store it in a variable.\n",
    "avg_salary = spark.sql(\"SELECT AVG(Salary) as avg_salary FROM employees\").first().avg_salary\n",
    "\n",
    "# Then, write the main query, using the average salary in the WHERE clause.\n",
    "df5 = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        a.*,\n",
    "        e.Name AS Employee_Name, \n",
    "        e.Company AS Employee_Company, \n",
    "        d.Department_Name, \n",
    "        d.Company AS Department_Company,\n",
    "        p.Project_Name, \n",
    "        p.Company AS Project_Company, \n",
    "        e.Salary AS Employee_Salary\n",
    "    FROM \n",
    "        assignments a\n",
    "    LEFT JOIN \n",
    "        employees e ON a.Employee_ID = e.ID\n",
    "    LEFT JOIN \n",
    "        departments d ON a.Department_ID = d.Department_ID\n",
    "    LEFT JOIN \n",
    "        projects p ON a.Project_ID = p.Project_ID\n",
    "    WHERE e.Salary > {avg_salary}\n",
    "\"\"\")\n",
    "\n",
    "df5.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-------------+----------+-------------+----------------+---------------+------------------+------------+---------------+---------------+\n",
      "|Assignment_ID|Employee_ID|Department_ID|Project_ID|Employee_Name|Employee_Company|Department_Name|Department_Company|Project_Name|Project_Company|Employee_Salary|\n",
      "+-------------+-----------+-------------+----------+-------------+----------------+---------------+------------------+------------+---------------+---------------+\n",
      "|26           |26         |2            |7         |Employee_26  |Company_2       |Department_2   |Company_2         |Project_7   |Company_1      |130000         |\n",
      "|30           |30         |6            |11        |Employee_30  |Company_0       |Department_6   |Company_0         |Project_11  |Company_2      |150000         |\n",
      "|18           |18         |24           |29        |Employee_18  |Company_0       |Department_24  |Company_0         |Project_29  |Company_2      |90000          |\n",
      "|22           |22         |28           |3         |Employee_22  |Company_1       |Department_28  |Company_1         |Project_3   |Company_0      |110000         |\n",
      "|19           |19         |25           |30        |Employee_19  |Company_1       |Department_25  |Company_1         |Project_30  |Company_0      |95000          |\n",
      "+-------------+-----------+-------------+----------+-------------+----------------+---------------+------------------+------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Another approach - without calling avg_salary variable \n",
    "\n",
    "df5 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        main.* \n",
    "    FROM \n",
    "        (\n",
    "            SELECT \n",
    "                a.*,\n",
    "                e.Name AS Employee_Name, \n",
    "                e.Company AS Employee_Company, \n",
    "                d.Department_Name, \n",
    "                d.Company AS Department_Company,\n",
    "                p.Project_Name, \n",
    "                p.Company AS Project_Company, \n",
    "                e.Salary AS Employee_Salary\n",
    "            FROM \n",
    "                assignments a\n",
    "            LEFT JOIN \n",
    "                employees e ON a.Employee_ID = e.ID\n",
    "            LEFT JOIN \n",
    "                departments d ON a.Department_ID = d.Department_ID\n",
    "            LEFT JOIN \n",
    "                projects p ON a.Project_ID = p.Project_ID\n",
    "        ) as main\n",
    "    JOIN \n",
    "        (SELECT AVG(Salary) as avg_salary FROM employees) as sub \n",
    "    WHERE main.Employee_Salary > sub.avg_salary -- Filtering out employees that have greater than average salary\n",
    "\"\"\")\n",
    "\n",
    "df5.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Window Functions\n",
    "--------------------------------------------\n",
    "\n",
    "#### Aggregate Window Functions: These functions perform a calculation on a set of rows and return a single value for each row. The syntax for aggregate window functions is identical to that used with other aggregate functions. Some of the commonly used aggregate window functions are: SUM(), AVG(), MIN(), MAX(), COUNT()\n",
    "\n",
    "#### Ranking Window Functions: These functions provide a way to rank rows in each partition. Each row in each partition is assigned a unique rank number. Some of the commonly used ranking window functions are: RANK(), DENSE_RANK(), ROW_NUMBER(), NTILE(), CUME_DIST(), PERCENT_RANK(), LEAD(), LAG(), FIRST_VALUE(), LAST_VALUE(), NTH_VALUE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-----------+\n",
      "|ID |Name       |Salary|Salary_Rank|\n",
      "+---+-----------+------+-----------+\n",
      "|30 |Employee_30|150000|1          |\n",
      "|29 |Employee_29|145000|2          |\n",
      "|28 |Employee_28|140000|3          |\n",
      "|27 |Employee_27|135000|4          |\n",
      "|26 |Employee_26|130000|5          |\n",
      "|25 |Employee_25|125000|6          |\n",
      "|24 |Employee_24|120000|7          |\n",
      "|23 |Employee_23|115000|8          |\n",
      "|22 |Employee_22|110000|9          |\n",
      "|21 |Employee_21|105000|10         |\n",
      "+---+-----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_salary_rank = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ID,\n",
    "        Name, \n",
    "        Salary,\n",
    "        RANK() OVER (ORDER BY Salary DESC) as Salary_Rank\n",
    "    FROM employees\n",
    "    ORDER BY Salary_Rank\n",
    "    LIMIT 10\n",
    "                           \n",
    "\"\"\")\n",
    "\n",
    "df_salary_rank.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

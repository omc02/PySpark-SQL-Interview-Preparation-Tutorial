{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Notebook Information '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Notebook Information \"\"\"\n",
    "# References: https://www.youtube.com/watch?v=m1KcNV-Zhmc&ab_channel=AlexTheAnalyst ; https://github.com/AlexTheAnalyst/SQL-Code\n",
    "# Workbook Title: PySpark Joins Tutorial - PySpark and SQL syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Setup\n",
    "--------------------------------------------\n",
    "#### Installing relevant libraries; Instantiating a PySpark session; Creating a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Importing libraries \"\"\"\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Instantiate a SparkContext \"\"\"\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "## Print the Spark version\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Creating a SparkSession \"\"\" \n",
    "spark = SparkSession.builder.appName('AdvancedSQL_Tutorial').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Dataframes\n",
    "--------------------------------------------\n",
    "#### Creating tables as PySpark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Creating EmployeeDemographics table \"\"\"\n",
    "# list  of EmployeeDemographics data\n",
    "data1 = [\n",
    "        \n",
    "        [1001, 'Jim', 'Halpert', 30, 'Male'],\n",
    "        [1002, 'Pam', 'Beasley', 30, 'Female'],\n",
    "        [1003, 'Dwight', 'Schrute', 29, 'Male'],\n",
    "        [1004, 'Angela', 'Martin', 31, 'Female'],\n",
    "        [1005, 'Toby', 'Flenderson', 32, 'Male'],\n",
    "        [1006, 'Michael', 'Scott', 35, 'Male'],\n",
    "        [1007, 'Meredith', 'Palmer', 32, 'Female'],\n",
    "        [1008, 'Stanley', 'Hudson', 38, 'Male'],\n",
    "        [1009, 'Kevin', 'Malone', 31, 'Male']\n",
    "        \n",
    "        ]\n",
    "\n",
    "# specify column names\n",
    "columns = ['EmployeeID', 'FirstName', 'LastName', 'Age', 'Gender']\n",
    "  \n",
    "# creating a df1 from the lists of data\n",
    "df1 = spark.createDataFrame(data1, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Creating EmployeeSalary table \"\"\"\n",
    "# list  of EmployeeSalary data\n",
    "data2 = [\n",
    "        \n",
    "        [1001, 'Salesman', 45000],\n",
    "        [1002, 'Receptionist', 36000],\n",
    "        [1003, 'Salesman', 63000],\n",
    "        [1004, 'Accountant', 47000],\n",
    "        [1005, 'HR', 50000],\n",
    "        [1006, 'Regional Manager', 65000],\n",
    "        [1007, 'Supplier Relations', 41000],\n",
    "        [1008, 'Salesman', 48000],\n",
    "        [1009, 'Accountant', 42000]\n",
    "        \n",
    "        ]\n",
    "\n",
    "# specify column names\n",
    "columns = ['EmployeeID', 'JobTitle', 'Salary']\n",
    "  \n",
    "# creating a df1 from the lists of data\n",
    "df2 = spark.createDataFrame(data2, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Creating Temporary Tables to be used in spark.sql \"\"\"\n",
    "## setting temporary views by creating dataframes\n",
    "# creating a view for df1 named Manufacturers\n",
    "df1.createOrReplaceTempView(\"EmployeeDemographics\")\n",
    "  \n",
    "# creating a view for df2 named Providers\n",
    "df2.createOrReplaceTempView(\"EmployeeSalary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries \n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---+------+\n",
      "|EmployeeID|FirstName|LastName  |Age|Gender|\n",
      "+----------+---------+----------+---+------+\n",
      "|1001      |Jim      |Halpert   |30 |Male  |\n",
      "|1002      |Pam      |Beasley   |30 |Female|\n",
      "|1003      |Dwight   |Schrute   |29 |Male  |\n",
      "|1004      |Angela   |Martin    |31 |Female|\n",
      "|1005      |Toby     |Flenderson|32 |Male  |\n",
      "|1006      |Michael  |Scott     |35 |Male  |\n",
      "|1007      |Meredith |Palmer    |32 |Female|\n",
      "|1008      |Stanley  |Hudson    |38 |Male  |\n",
      "|1009      |Kevin    |Malone    |31 |Male  |\n",
      "+----------+---------+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Checking the PySpark dataframe \"\"\"\n",
    "\n",
    "# checking the dataframe  - EmployeeDemographics Table \n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------+\n",
      "|EmployeeID|JobTitle          |Salary|\n",
      "+----------+------------------+------+\n",
      "|1001      |Salesman          |45000 |\n",
      "|1002      |Receptionist      |36000 |\n",
      "|1003      |Salesman          |63000 |\n",
      "|1004      |Accountant        |47000 |\n",
      "|1005      |HR                |50000 |\n",
      "|1006      |Regional Manager  |65000 |\n",
      "|1007      |Supplier Relations|41000 |\n",
      "|1008      |Salesman          |48000 |\n",
      "|1009      |Accountant        |42000 |\n",
      "+----------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Checking the PySpark dataframe \"\"\"\n",
    "\n",
    "# checking the dataframe  - EmployeeSalary Table \n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------------+\n",
      "|EmployeeID|Salary|All_average_salary|\n",
      "+----------+------+------------------+\n",
      "|1001      |45000 |48555.555555555555|\n",
      "|1002      |36000 |48555.555555555555|\n",
      "|1003      |63000 |48555.555555555555|\n",
      "|1004      |47000 |48555.555555555555|\n",
      "|1005      |50000 |48555.555555555555|\n",
      "|1006      |65000 |48555.555555555555|\n",
      "|1007      |41000 |48555.555555555555|\n",
      "|1008      |48000 |48555.555555555555|\n",
      "|1009      |42000 |48555.555555555555|\n",
      "+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Subqueries - In SELECT FROM and WHERE Statements \"\"\"\n",
    "\n",
    "# Get the employee ID, Salary and average salary of all employees\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT EmployeeID, Salary, (SELECT AVG(Salary) FROM EmployeeSalary) AS All_average_salary\n",
    "        FROM EmployeeSalary\n",
    "        GROUP BY EmployeeID, Salary\n",
    "        Order BY 1,2 \n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)\n",
    "\n",
    "# Note: Order BY 1,2 sorts by the first and second column which is equivalent to sorting based on EmployeeID and Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------------+-------------------+\n",
      "|EmployeeID|Salary|All_average_salary|Salary_difference  |\n",
      "+----------+------+------------------+-------------------+\n",
      "|1001      |45000 |48555.555555555555|-3555.5555555555547|\n",
      "|1002      |36000 |48555.555555555555|-12555.555555555555|\n",
      "|1003      |63000 |48555.555555555555|14444.444444444445 |\n",
      "|1004      |47000 |48555.555555555555|-1555.5555555555547|\n",
      "|1005      |50000 |48555.555555555555|1444.4444444444453 |\n",
      "|1006      |65000 |48555.555555555555|16444.444444444445 |\n",
      "|1007      |41000 |48555.555555555555|-7555.555555555555 |\n",
      "|1008      |48000 |48555.555555555555|-555.5555555555547 |\n",
      "|1009      |42000 |48555.555555555555|-6555.555555555555 |\n",
      "+----------+------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Subqueries - In the SELECT statement \"\"\"\n",
    "\n",
    "# Get the employee ID and salaries of each employee along with the difference between the employee salary and average salary of all employees\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT EmployeeID, Salary, (SELECT AVG(Salary) FROM EmployeeSalary) AS All_average_salary, Salary - (SELECT AVG(Salary) FROM EmployeeSalary) AS Salary_difference\n",
    "        FROM EmployeeSalary\n",
    "        GROUP BY EmployeeID, Salary\n",
    "        Order BY 1,2\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)\n",
    "\n",
    "# Note: Order BY 1,2 means that we are orderding the values based on the EmployeeID and Salary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|EmployeeID|AllAvgSalary      |\n",
      "+----------+------------------+\n",
      "|1001      |48555.555555555555|\n",
      "|1002      |48555.555555555555|\n",
      "|1003      |48555.555555555555|\n",
      "|1004      |48555.555555555555|\n",
      "|1005      |48555.555555555555|\n",
      "|1006      |48555.555555555555|\n",
      "|1007      |48555.555555555555|\n",
      "|1008      |48555.555555555555|\n",
      "|1009      |48555.555555555555|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Subqueries - In the FROM statement \"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT A.EmployeeID, A.AllAvgSalary\n",
    "        FROM (SELECT EmployeeID, Salary, AVG(Salary) over () AllAvgSalary\n",
    "              FROM EmployeeSalary) AS A \n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------+\n",
      "|EmployeeID|JobTitle          |Salary|\n",
      "+----------+------------------+------+\n",
      "|1009      |Accountant        |42000 |\n",
      "|1007      |Supplier Relations|41000 |\n",
      "|1005      |HR                |50000 |\n",
      "|1008      |Salesman          |48000 |\n",
      "|1004      |Accountant        |47000 |\n",
      "|1006      |Regional Manager  |65000 |\n",
      "+----------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Subqueries - In WHERE statement \"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT EmployeeID, JobTitle, Salary\n",
    "        FROM EmployeeSalary \n",
    "        WHERE EmployeeID IN (SELECT EmployeeID\n",
    "                             FROM EmployeeDemographics\n",
    "                             WHERE Age > 30)\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)\n",
    "\n",
    "# Note: You can also perform the above query using a join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---+------+\n",
      "|EmployeeID|FirstName|LastName  |Age|Gender|\n",
      "+----------+---------+----------+---+------+\n",
      "|1001      |Jim      |Halpert   |30 |Male  |\n",
      "|1002      |Pam      |Beasley   |30 |Female|\n",
      "|1003      |Dwight   |Schrute   |29 |Male  |\n",
      "|1004      |Angela   |Martin    |31 |Female|\n",
      "|1005      |Toby     |Flenderson|32 |Male  |\n",
      "+----------+---------+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Subqueries - In FROM statement \"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM EmployeeDemographics\n",
    "        LIMIT 5\n",
    "       \"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7e022b67859ae4a791bbcc1c75bde8b3a5bef3b9abb0060fdb4399d638fb2f6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
